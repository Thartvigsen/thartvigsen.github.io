---
permalink: /
title: ""
excerpt: "About me" 
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!--
Hi! I received my PhD from Worcester Polytechnic Institute where I worked with [Elke Rundensteiner](https://www.wpi.edu/people/faculty/rundenst) and [Xiangnan Kong](https://web.cs.wpi.edu/~xkong/).

I am starting at MIT as a postdoc working with [Marzyeh Ghassemi](http://www.marzyehghassemi.com/) in January.
-->

<!--
Hi! I am beginning as a postdoc at [MIT](https://www.csail.mit.edu/) in January, doing machine learning research with Marzyeh Ghassemi. I aim to build systems that learn from large amounts of data to support clinical decision making in ongoing environments without perpetuating societal biases.

Hi! I'm a postdoc at [MIT](https://www.csail.mit.edu/) doing machine learning research at CSAIL with Marzyeh Ghassemi. In my research, I aim to build systems that learn from large amounts of data to support clinical decision making in ongoing environments without perpetuating societal biases.

I received my PhD from Worcester Polytechnic Institute where I worked with [Elke Rundensteiner](https://www.wpi.edu/people/faculty/rundenst) and [Xiangnan Kong](https://web.cs.wpi.edu/~xkong/).
-->
### About me
I am a postdoc at MIT in the Computer Science and Artificial Intelligence Laboratory ([CSAIL](https://www.csail.mit.edu/)).

My research focuses on machine learning, data mining, time series, applications to healthcare, NLP, and fairness in AI systems.

At MIT, I work with Professor Marzyeh Ghassemi as a member of the [HealthyML lab](https://healthyml.org/).

<!--
postdoc at [MIT CSAIL](https://www.csail.mit.edu/) doing machine learning research with Professor [Marzyeh Ghassemi](https://healthyml.org/).
To this end, I develop **data mining** and **machine learning** models and tools mainly for **time series** and **text**.
-->

### Research
I build machine learning and data mining systems to support time-sensitive decision making without perpetuating societal biases.
The projects that excite me the most: (1) provide robust models of ongoing systems, often through partially-observed time series, (2) defend users from machine bias, and (3) have impact through real-world deployment.

Some of my recent projects include:
<ul style="margin-bottom: 1px;">
  <li>ToxiGen, a dataset for detecting implicitly toxic language that targets disadvantaged groups. These data are hard to collect in the wild, so we trick pre-trained language models into generating it for us automatically! (paper coming soon)</li>
  <li>Systems for learning to stop and classify ongoing time series early in time-sensitive domains (see <a href="https://thartvigsen.github.io/papers/kdd19.pdf">KDD'19</a> and <a href="https://thartvigsen.github.io/papers/kdd20.pdf">KDD'20</a> papers).</li>
  <li>Explaining black-box models for time series and natural language processing (see <a href="https://thartvigsen.github.io/papers/cikm21.pdf">CIKM'21</a> and <a href="https://thartvigsen.github.io/papers/acl20.pdf">ACL'20</a> papers).</li>
  <li>Methods for recovering models of annotators' labeling behavior from machine learning datasets (see <a href="papers/sdm22.pdf">AAAI'22</a> and <a href="papers/sdm22.pdf">SDM'22</a> papers).</li>
</ul>

All projects are joint work. For details and credit to all authors, please visit my [full list of publications](https://thartvigsen.github.io/publications/).

<!--
In my research, I study **data mining** and **machine learning** for **time series** and **text**.
-->

<!--
My work has been published in several top papers (KDD, NeurIPS, AAAI, ACL) and I spent a year collaborating with the UMass Medical School using machine learning to help doctors write better clinical trials faster.

My research is funded by a [GAANN Research Fellowship](https://www2.ed.gov/programs/gaann/index.html) and with robust methods in hand, I aim to improve **healthcare**.

Please feel free to contact me, I am always happy to chat!

### Research Interests
<ul style="margin-bottom: 5px;">
<small>
  <li><b>Time Series</b> (see our KDD<a href="https://thartvigsen.github.io/papers/kdd19.pdf">'19</a>, <a href="https://thartvigsen.github.io/papers/kdd20.pdf">'20</a>, <a href="https://thartvigsen.github.io/papers/kdd21.pdf">'21</a> and <a href="https://thartvigsen.github.io/papers/kdd20.pdf">SDM'22</a> papers)</li>
  <li><b>Recurrent Neural Networks</b> (see our <a href="https://thartvigsen.github.io/papers/neurips21.pdf">NeurIPS 2021</a>, <a href="https://thartvigsen.github.io/papers/aaai21.pdf">AAAI 2021</a>, and <a href="https://thartvigsen.github.io/papers/cikm20.pdf">CIKM 2020</a> papers)</li>
  <li><b>Auditing Machine Learning Systems</b> (see our <a href="https://thartvigsen.github.io/papers/acl20.pdf">ACL 2020</a> and <a href="https://thartvigsen.github.io/papers/cikm21.pdf">CIKM 2021</a> papers)</li>
  <li><b>Healthcare</b> (see our <a href="http://ecmlpkdd2017.ijs.si/papers/paperID487.pdf">ECML 2017</a>, <a href="https://www.scitepress.org/Papers/2018/65996/65996.pdf">HEALTHINF 2018</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9006400">BIGDATA 2019</a>, and <a href="https://ieeexplore.ieee.org/abstract/document/9006403">BIGDATA 2019</a> papers)</li>
</small>
</ul>
-->

<!--
Current interests:
- Auditing pre-trained models: How can I tell what *general properties* of input data will convince my model? How do I correct a pre-trained model's bad behavior?
- Modeling data and label collection: Training data determine the behavior and performance of machine learning models. Which data instances get added to a dataset? Which ones get labeled? Modern models are data hungry so collection and labeling procedures have large influences on the behavior of a model. How can we model the influence of these mechanisms from data collection, to labeling, to eventual machine inference?
- Representing irregularly-sampled data: When data are collected irregularly, deep learning models don't work out of the box anymore because they require fixed-length inputs. However, despite a recent surge in methods that can reconstruct continuous-timelines (like NODEs), simpler models that just discretize time seem to work just as well (if not better). How can we tell when a continuous-time representation learner is required compared to a simpler discrete method?
- 
-->

<!--
<li> <span style="color: #E30B5C"><b>ACL'22 paper</b></span> on <a href="papers/acl22.pdf">detecting toxic language targeting minority groups.</a></li>
<li> Started as a postdoc at <span style="color: #E30B5C"><b>MIT CSAIL</b></span>.</li>
-->

### What's New?
<ul style="margin-bottom: 5px;">
<small>
<li> <span style="color: #E30B5C"><b>SDM'22 paper</b></span> on <a href="papers/sdm22.pdf">positive unlabeled learning.</a></li>
<li> <span style="color: #E30B5C"><b>AAAI'22 paper</b></span> on <a href="papers/aaai22.pdf">positive unlabeled learning.</a></li>
<li> <span style="color: #E30B5C"><b>NeurIPS'21 paper</b></span> on <a href="papers/neurips21.pdf">multi-label learning.</a></li>
<li> <span style="color: #E30B5C"><b>CIKM'21 paper</b></span> on <a href="papers/cikm21.pdf">explainability for time series.</a></li>
<li> Summer 2021 internship with <span style="color: #E30B5C"><b>Microsoft</b></span>.</li>
<li> <span style="color: #E30B5C"><b>KDD'21 paper</b></span> on <a href="papers/kdd21.pdf">spike train classification</a>.</li>
</small>
</ul>

### History
Before MIT, I did my PhD in Data Science at Worcester Polytechnic Institute, advised by Professors [Elke Rundensteiner](https://www.wpi.edu/people/faculty/rundenst) and [Xiangnan Kong](https://web.cs.wpi.edu/~xkong/).
Before that, I graduated from [SUNY Geneseo](https://www.geneseo.edu/) in 2016 with a B.A. in Applied Mathematics.

### Misc

Outside of research, I enjoy [bouldering](/images/climbing2.jpg), [biking](/images/bike.jpg), books (science fiction/science fact), birding, juggling, and playing guitar.
I also spent a summer living at [BioSphere 2](https://en.wikipedia.org/wiki/Biosphere_2) in rural Arizona.
