---
title: "Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?"
collection: publications
excerpt: 'This paper studies the interpretability of attention mechanisms in RNNs for Text Classification and describes a new and publicly-available dataset containing crowd-sourced human attention maps.'
venue: ACL
date: 2020-07-05
---

Attention mechanisms have dramatically improved the accuracy of recent RNN-based NLP methods. There are many claims that they add interpretability to these methods. Recently, this has sparked a debate within the NLP community with many recent works studying the nature of said interpretability. However, they rarely use human-validation in the definition of interpretability. Thus, in this work, we collect a large human-annotation dataset, allowing us to evaluate *to what degree do human attention maps agree with machine attention maps across a variety of attention mechanisms?*. We ultimately find that different architectures compare more or less to human-annotations, lending insights into which mechanisms should be considered more interpretable. We also release our newly-collected dataset in conjunction with this paper so that future researchers may use it to evaluate their attention models.

```
@inproceedings{sen2020human,
  title={Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?},
  author={Sen, Cansu and Hartvigsen, Thomas and Yin, Biao and Kong, Xiangnan and Rundensteiner, Elke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}
```
